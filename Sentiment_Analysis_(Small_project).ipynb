{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "dhidF6uUssXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload your file to your Google Drive folder and load it\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd 'drive/My Drive/Colab Notebooks'\n",
        "except ImportError as e:\n",
        "    pass"
      ],
      "metadata": {
        "id": "xw05qwl1wWWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to set the seed for ensuring the generated result will be exactly same in every execution\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "yG7kttE0wWvM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the dataset into this jupyter file\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Lx7HFdLKwZuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Data"
      ],
      "metadata": {
        "id": "6SZhQiI-2iun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_sentence = []\n",
        "\n",
        "# tokenize the sentence\n",
        "for i in range (len(df)):\n",
        "  tokenized_sentence.append(word_tokenize(df.loc[i, 'Sentence']))\n",
        "\n",
        "df['tokenized sentence'] = tokenized_sentence"
      ],
      "metadata": {
        "id": "3XJd8mjZwogj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the length of the sentence\n",
        "for i in range (len(df)):\n",
        "  df.loc[i, 'length of sentence'] = int(len(df.loc[i, 'tokenized sentence']))\n",
        "\n",
        "# view the distribution of the sentence length\n",
        "plt.hist(df['length of sentence'])\n",
        "plt.title('The histogram of sentence length after tokenizing')\n",
        "plt.xlabel('Length of sentence')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AsOEeUnRx09O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = df['length of sentence'].max()\n",
        "print(f\"The maximum length of the sentence is {max_words}\")"
      ],
      "metadata": {
        "id": "vdDBWIBX0LN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view whether the data is imbalance or not\n",
        "import seaborn as sns\n",
        "sns.countplot(x ='Sentiment', data = df)\n",
        "plt.title('Distribution of the sentiment attributes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgpf2eHa1oS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing on Data"
      ],
      "metadata": {
        "id": "wh8GHcOb2mdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing punctuation from the given sentence\n",
        "import string\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# transform label variables into ordinal variable\n",
        "labelEncoder = preprocessing.LabelEncoder()\n",
        "df['Sentiment'] = labelEncoder.fit_transform(df['Sentiment'])\n",
        "\n",
        "df['Sentence'] = df['Sentence'].str.lower()\n",
        "df['Sentence'] = df['Sentence'].str.replace('\\d+', '')\n",
        "for char in string.punctuation:\n",
        "    df['Sentence'] = df['Sentence'].str.replace(char, ' ')"
      ],
      "metadata": {
        "id": "jMlxKvaw7yuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = []\n",
        "\n",
        "# tokenize the sentence\n",
        "for i in range (len(df)):\n",
        "  tokenized_sentence.append(word_tokenize(df.loc[i, 'Sentence']))\n",
        "\n",
        "df['tokenized sentence'] = tokenized_sentence"
      ],
      "metadata": {
        "id": "Q2JGrX1v88uS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords from the given sentences\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "without_stopwords = []\n",
        "for index in range (len(df)):\n",
        "  tokenized_sentence = df.loc[index,'tokenized sentence']\n",
        "  temp_filt = []\n",
        "  for token in tokenized_sentence:\n",
        "    if (token not in stop_words):\n",
        "      temp_filt.append(token)\n",
        "\n",
        "  without_stopwords.append(temp_filt)\n",
        "\n",
        "df['tokenized sentence'] = without_stopwords"
      ],
      "metadata": {
        "id": "3mDd_u__186Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# generate pos tagging for each token\n",
        "for index, row in df.iterrows():\n",
        "    tokens = row['tokenized sentence']\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    df.at[index, 'tokenized sentence'] = pos_tags"
      ],
      "metadata": {
        "id": "n_ttzcmKANOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# define the pos tagging that refer to the wordnet for lemmatization\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "1QN0wvKH-Kfs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# doing lemmatization on those given tokens within each sentence\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for index in range(len(df)):\n",
        "    for token in range(len(df.loc[index, 'tokenized sentence'])):\n",
        "        word = df.loc[index, 'tokenized sentence'][token][0]\n",
        "        pos = df.loc[index, 'tokenized sentence'][token][1]\n",
        "        wordnet_pos = get_wordnet_pos(pos)\n",
        "        if wordnet_pos is not None:\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(word)\n",
        "        df.loc[index, 'tokenized sentence'][token] = (lemma, pos)"
      ],
      "metadata": {
        "id": "Ho_ED3_wD3uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the pos tagging from the tokens\n",
        "for index in range (len(df)):\n",
        "  for token in range (len(df.loc[index, 'tokenized sentence'])):\n",
        "    df.loc[index, 'tokenized sentence'][token] = str(df.loc[index, 'tokenized sentence'][token]).split(',')[0][2:-1]"
      ],
      "metadata": {
        "id": "mO9L2dXjgc8C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detokenizer the sentence back to sentence\n",
        "!pip install keras_preprocessing\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "for index in range (len(df)):\n",
        "  df.loc[index, 'Sentence'] = TreebankWordDetokenizer().detokenize(df.loc[index, 'tokenized sentence'])"
      ],
      "metadata": {
        "id": "U9ZhscIkpngo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the vocabolary size of the tokenized sentence\n",
        "%%time\n",
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in df[\"tokenized sentence\"].values:\n",
        "    for word in text:\n",
        "        cnt[word] += 1\n",
        "\n",
        "vocab_size = len(cnt)"
      ],
      "metadata": {
        "id": "z6GrdSMKfpB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# splitting the data into x and y for preparing training\n",
        "# converting the dataframe's column into list\n",
        "X = df.drop(columns = ['Sentiment', 'length of sentence', 'tokenized sentence'])\n",
        "X = X['Sentence'].values.tolist()\n",
        "X = np.array(X)\n",
        "y = df['Sentiment']\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "bXa9G4cmdPvj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the sentence and set to use the 8664 vocabulary (since we already removed the stopwords from those given sentences)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 8664, lower = True)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# compute the index for each token\n",
        "X = tokenizer.texts_to_sequences(X)"
      ],
      "metadata": {
        "id": "CukW45YtvJJ9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "# adding pad into each sentence after tokenizing\n",
        "max_length = 81\n",
        "X = pad_sequences(X, padding = 'post', maxlen = max_length)"
      ],
      "metadata": {
        "id": "P9MEiXSHvJMZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# splitting data into train test\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.1)\n"
      ],
      "metadata": {
        "id": "BK1i8_OfZYDx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter optimization (RNN)"
      ],
      "metadata": {
        "id": "j9onRAfX8xY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Optuna\n",
        "\n",
        "import optuna\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "cDulkoQ17zqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    embd_len = trial.suggest_categorical('embd_len', [32, 64, 128])\n",
        "    rnn_units = trial.suggest_categorical('rnn_units', [64, 128, 256])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    epochs = trial.suggest_int('epochs', 5, 25)\n",
        "\n",
        "\n",
        "    # Create the RNN model with the suggested hyperparameters\n",
        "    model = Sequential(name = \"Simple_RNN\")\n",
        "    model.add(Embedding(vocab_size, embd_len, input_length = max_words))\n",
        "    model.add(SimpleRNN(rnn_units, activation = 'tanh'))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate), metrics = ['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train, batch_size=64, epochs = epochs, verbose = 1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_pred = model.predict(x_valid)\n",
        "    accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Define the study object for Optuna\n",
        "study = optuna.create_study(direction = 'maximize')\n",
        "\n",
        "# Start the hyperparameter optimization\n",
        "study.optimize(objective, n_trials = 25)\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "best_params = study.best_params\n",
        "best_accuracy = study.best_value\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "id": "JwxMDVn77zr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model (Recurrent Neural Network)"
      ],
      "metadata": {
        "id": "KeBWh_P-86pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing every word's embedding size to be 64\n",
        "embd_len = 128\n",
        "\n",
        "# vocab_size is the unique words within the given sentence\n",
        "# embd_len is the dimensional of the word embedding for each token\n",
        "# input_length is the size of the sentence for inserting into the model\n",
        "\n",
        "# Creating a RNN model\n",
        "RNN_model = Sequential(name=\"Simple_RNN\")\n",
        "RNN_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\tinput_length=max_length))\n",
        "\n",
        "# In case of a stacked(more than one layer of RNN)\n",
        "# use return_sequences=True\n",
        "# simpleRNN is each layer as known as the state of the RNN\n",
        "RNN_model.add(SimpleRNN(64,\n",
        "\t\t\t\t\t\tactivation='tanh',\n",
        "\t\t\t\t\t\treturn_sequences=False))\n",
        "\n",
        "# dense be the last layer for classifying those embedding into according sentiment\n",
        "RNN_model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# printing model summary\n",
        "print(RNN_model.summary())\n",
        "\n",
        "# Compiling model\n",
        "# Stochastic Gradient Descent for back propagation\n",
        "RNN_model.compile(\n",
        "\tloss=\"binary_crossentropy\",\n",
        "\toptimizer=Adam(learning_rate = 0.006185314591866337),\n",
        "\tmetrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history = RNN_model.fit(x_train, y_train,\n",
        "\t\t\t\t\t\tbatch_size = 64,\n",
        "\t\t\t\t\t\tepochs = 4,\n",
        "\t\t\t\t\t\tverbose = 1,\n",
        "\t\t\t\t\t\tvalidation_data = (x_valid, y_valid))\n",
        "\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"Simple_RNN Score---> \", RNN_model.evaluate(x_test, y_test, verbose = 1))\n"
      ],
      "metadata": {
        "id": "AQ8r7eNxiHYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization (LSTM)"
      ],
      "metadata": {
        "id": "I-STG3zlTJ9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    embd_len = trial.suggest_categorical('embd_len', [32, 64, 128])\n",
        "    rnn_units = trial.suggest_categorical('lstm_units', [64, 128, 256])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    epochs = trial.suggest_int('epochs', 5, 25)\n",
        "\n",
        "\n",
        "    # Create the RNN model with the suggested hyperparameters\n",
        "    model = Sequential(name = \"LSTM_Model\")\n",
        "    model.add(Embedding(vocab_size, embd_len, input_length = max_length))\n",
        "    model.add(LSTM(rnn_units, activation = 'tanh'))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate), metrics = ['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train, batch_size = 64, epochs = epochs, verbose = 1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_pred = model.predict(x_valid)\n",
        "    accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Define the study object for Optuna\n",
        "study = optuna.create_study(direction = 'maximize')\n",
        "\n",
        "# Start the hyperparameter optimization\n",
        "study.optimize(objective, n_trials = 25)\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "best_params = study.best_params\n",
        "best_accuracy = study.best_value\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "id": "KaEfzy_aTInT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training (Long-Short Term Memory)"
      ],
      "metadata": {
        "id": "BrhzqIOpLhKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining LSTM model\n",
        "lstm_model = Sequential(name = \"LSTM_Model\")\n",
        "lstm_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\tinput_length = max_length))\n",
        "lstm_model.add(LSTM(128,\n",
        "\t\t\t\t\tactivation = 'relu',\n",
        "\t\t\t\t\treturn_sequences = False))\n",
        "lstm_model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# Printing Model Summary\n",
        "print(lstm_model.summary())\n",
        "\n",
        "# Compiling the model\n",
        "lstm_model.compile(\n",
        "\tloss = \"binary_crossentropy\",\n",
        "\toptimizer = 'adam',\n",
        "\tmetrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history3 = lstm_model.fit(x_train, y_train,\n",
        "\t\t\t\t\t\tbatch_size = 64,\n",
        "\t\t\t\t\t\tepochs = 5,\n",
        "\t\t\t\t\t\tverbose = 2,\n",
        "\t\t\t\t\t\tvalidation_data = (x_valid, y_valid))\n",
        "\n",
        "# Displaying the model accuracy on test data\n",
        "print()\n",
        "print(\"LSTM model Score---> \", lstm_model.evaluate(x_test, y_test, verbose = 1))\n"
      ],
      "metadata": {
        "id": "UrEBp3IhiHcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization (Bi-LSTM)"
      ],
      "metadata": {
        "id": "EC9KRTjUTQWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    embd_len = trial.suggest_categorical('embd_len', [32, 64, 128])\n",
        "    rnn_units = trial.suggest_categorical('bi_lstm_units', [64, 128, 256])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    epochs = trial.suggest_int('epochs', 5, 25)\n",
        "\n",
        "\n",
        "    # Create the RNN model with the suggested hyperparameters\n",
        "    model = Sequential(name = \"Bidirectional_LSTM\")\n",
        "    model.add(Embedding(vocab_size, embd_len, input_length = max_length))\n",
        "    model.add(Bidirectional(LSTM(rnn_units, activation = 'tanh')))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate), metrics = ['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train, batch_size = 64, epochs = epochs, verbose=1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_pred = model.predict(x_valid)\n",
        "    accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Define the study object for Optuna\n",
        "study = optuna.create_study(direction = 'maximize')\n",
        "\n",
        "# Start the hyperparameter optimization\n",
        "study.optimize(objective, n_trials = 25)\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "best_params = study.best_params\n",
        "best_accuracy = study.best_value\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "id": "CwtoKXuWTThK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model (Bi-LSTM)"
      ],
      "metadata": {
        "id": "u6E4cTRMPL6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Bidirectional LSTM model\n",
        "bi_lstm_model = Sequential(name = \"Bidirectional_LSTM\")\n",
        "bi_lstm_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\t\tinput_length = max_length))\n",
        "\n",
        "bi_lstm_model.add(Bidirectional(LSTM(128,\n",
        "\t\t\t\t\t\t\t\t\tactivation = 'tanh',\n",
        "\t\t\t\t\t\t\t\t\treturn_sequences = False)))\n",
        "bi_lstm_model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# Printing model summary\n",
        "print(bi_lstm_model.summary())\n",
        "\n",
        "# Compiling model summary\n",
        "bi_lstm_model.compile(\n",
        "loss=\"binary_crossentropy\",\n",
        "optimizer = 'adam',\n",
        "metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history4 = bi_lstm_model.fit(x_train, y_train,\n",
        "\t\t\t\t\t\t\tbatch_size = 64,\n",
        "\t\t\t\t\t\t\tepochs = 5,\n",
        "\t\t\t\t\t\t\tverbose = 2,\n",
        "\t\t\t\t\t\t\tvalidation_data = (x_valid, y_valid))\n",
        "\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"Bidirectional LSTM model Score---> \",\n",
        "\tbi_lstm_model.evaluate(x_test, y_test, verbose = 1))\n"
      ],
      "metadata": {
        "id": "qCVArr1tLgkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on customizing activation function"
      ],
      "metadata": {
        "id": "PvEwENKmVLEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "def custom_activation(x):\n",
        "    # Define your custom activation function logic\n",
        "    return tf.square(tf.sin(x))\n",
        "\n",
        "# Defining Bidirectional LSTM model\n",
        "bi_lstm_model = Sequential(name = \"Bidirectional_LSTM\")\n",
        "bi_lstm_model.add(Embedding(vocab_size,\n",
        "\t\t\t\t\t\t\tembd_len,\n",
        "\t\t\t\t\t\t\tinput_length = max_length))\n",
        "\n",
        "bi_lstm_model.add(Bidirectional(LSTM(128,\n",
        "\t\t\t\t\t\t\t\t\tactivation = 'tanh',\n",
        "\t\t\t\t\t\t\t\t\treturn_sequences = False)))\n",
        "bi_lstm_model.add(Dense(1, activation = custom_activation))\n",
        "\n",
        "# Printing model summary\n",
        "print(bi_lstm_model.summary())\n",
        "\n",
        "# Compiling model summary\n",
        "bi_lstm_model.compile(\n",
        "loss = \"binary_crossentropy\",\n",
        "optimizer = 'adam',\n",
        "metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history4 = bi_lstm_model.fit(x_train, y_train,\n",
        "\t\t\t\t\t\t\tbatch_size = 64,\n",
        "\t\t\t\t\t\t\tepochs = 5,\n",
        "\t\t\t\t\t\t\tverbose = 2,\n",
        "\t\t\t\t\t\t\tvalidation_data = (x_valid, y_valid))\n",
        "\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"Bidirectional LSTM model Score---> \",\n",
        "\tbi_lstm_model.evaluate(x_test, y_test, verbose = 1))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tly-BqOEVKi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}